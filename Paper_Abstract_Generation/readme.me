# 使用模型：
meta-llama/Llama-3.1-8B-Instruct
--- 
# 如何復現，以下為復現用程式碼，需確保路徑./model_checkpoint/final_model 無誤(將程式碼及資料集放在313706034_code資料夾中)：
import json
import pandas as pd
import torch
from tqdm.notebook import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer
from peft import PeftModel
import os

# 載入測試數據
with open('test.json', 'r') as f:
    test_data = [json.loads(line) for line in f]
test_df = pd.DataFrame(test_data)

# 設置模型路徑
model_name = "meta-llama/Llama-3.1-8B-Instruct"
model_path = "./model_checkpoint/final_model"

# 初始化標記器
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token

# 設置量化配置
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_enable_fp32_cpu_offload=True
)

# 載入基礎模型
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)

# 載入微調模型
model = PeftModel.from_pretrained(base_model, model_path)

# 創建輸出文件
output_file = '313706034.json'
with open(output_file, 'w') as f:
    pass

# 生成預測
for i, intro in enumerate(tqdm(test_df["introduction"], desc="生成摘要")):
    # 準備提示文本
    prompt = f"Generate an abstract for the following paper introduction:\\n\\n{intro}\\n\\nAbstract:"
    
    # 編碼輸入
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1536).to(model.device)
    
    # 生成輸出
    output = model.generate(
        **inputs, 
        max_new_tokens=512,
        num_beams=2,
        no_repeat_ngram_size=3,
        early_stopping=False
    )
    
    # 解碼輸出
    pred_text = tokenizer.decode(output[0], skip_special_tokens=True)
    # 提取摘要部分
    abstract = pred_text.split("Abstract:")[-1].strip()
    
    # 寫入當前生成的摘要到文件
    with open(output_file, 'a') as f:
        json_line = {
            "paper_id": int(test_df.iloc[i]["paper_id"]),
            "abstract": str(abstract)
        }
        f.write(json.dumps(json_line) + '\n')
    
    # 釋放內存
    torch.cuda.empty_cache()

print(f"摘要生成完成，結果已保存到 {output_file}")